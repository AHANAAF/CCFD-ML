{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 23498,
          "sourceType": "datasetVersion",
          "datasetId": 310
        }
      ],
      "dockerImageVersionId": 30085,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Project on Credit Card Fraud Detection | Beginner",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'creditcardfraud:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F310%2F23498%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240823%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240823T080401Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D364f8e0fadce4201df2301edbff1948c1224961c76cfe15c517f052e13f41c209d7ed92a34024edbcd9ad301df0c734818f286020cda3a54a6c43cb4f2a86899f88e297b410f3f8141b542063f1becf7d735176044ec4ceb7a842da8c2d27a20ae63c8842ea605292ad96f6cdd27c664f6a9c785439b64a6f7544f465039ed346d2855658bce3a35af568ccefb18131bf52fc3ba4434812a2c57847b6733f07ae4a0f574c51f5e04d565da548a91cba93eb2c4a0b256ec5c9f31edfe7409c09b27a65f78991fee36649cdcda89db41749c11da8326358f71753f5e36dcf92639df74116b9be03bcb69febec0e8afc3aafc981ab6f6671ece11b17b84af4a89be'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "twCLDQddQ55u"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've referred to the notebook by Victoria Mendoza [https://www.kaggle.com/mendozav/credit-card-fraud-detection-project](http://) for this project.\n",
        " A big thanks to her for creating such a wonderful notebook!"
      ],
      "metadata": {
        "id": "cd7kAboMQ555"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Introduction\n",
        "\n",
        "I have recently started using Kaggle after completing the course \"Python for Machine Learning and Data Science Bootcamp\" on Udemy. Right now, I'm trying to do as many projects as I can before starting my Masters in Data Science for Fall 2021.\n",
        "\n",
        "I frequently refer to other people's submissions in my notebook while trying to build my own code. Although, I try to give as much credits as possible to the authors of various notebooks, however, if I've forgotten to give credit to someone, please accept my apologies in advance.\n",
        "\n",
        "So, without any further adeiu, lets get started!"
      ],
      "metadata": {
        "id": "cVwTURXsQ559"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 About the Dataset\n",
        "\n",
        "* The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
        "\n",
        "* This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n",
        "\n",
        "* The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "* It contains only numerical input variables which are the result of a PCA transformation.\n",
        "\n",
        "* Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data.\n",
        "\n",
        "* Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.\n",
        "\n",
        "* Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
        "\n",
        "* The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n",
        "\n",
        "* Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n"
      ],
      "metadata": {
        "id": "7WZWuLUSQ559"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Objective\n",
        "\n",
        "* The dataset contains a very minute percentage transacions, which are fraudulent. We need to find out those transactions which belong to the Fraud Class\n",
        "\n",
        "* Based on the data we have to generate a set of insights and recommendations that will help the credit card company from preventing the customers to be charged falsly!\n",
        "\n"
      ],
      "metadata": {
        "id": "z0Gx9KfmQ55_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Credit Card Fraud Detection Project\n",
        "#Date April 21, 2021"
      ],
      "metadata": {
        "trusted": true,
        "id": "xxUzGsQZQ55_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import scipy\n",
        "import sklearn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # To supress warnings\n",
        "sns.set(style=\"whitegrid\") # set the background for the graphs\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aJO2NnrkQ56A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing data from kaggle\n",
        "data = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\n",
        "data.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "x2SCYy5LQ56B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we do not know what actually does V1,V2....V28 means due to data confidentiality, but what we know is they're going to help us draw insights from the data."
      ],
      "metadata": {
        "id": "1k5fM1Z4Q56C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yaiyfDF_Q56D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8J915KpCQ56D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset does not contain any object data type, so we do not have to spend any time on conversion. Lets see if our data contains any null values!"
      ],
      "metadata": {
        "id": "dcUC2BrGQ56E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ET7mF81IQ56F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "JDqmhmfnQ56F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "e4sbBgVtQ56F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random_state helps assure that you always get the same output when you split the data\n",
        "# this helps create reproducible results and it does not actually matter what the number is\n",
        "# frac is percentage of the data that will be returned\n",
        "data = data.sample(frac = 0.2, random_state = 1)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "m73yWwY_Q56G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us conduct some exploratory data analysis on our data!"
      ],
      "metadata": {
        "id": "C3raDk2ZQ56H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the count of survivors\n",
        "sns.countplot('Class', data=data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "W-ZO5nJGQ56H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! The count of fraudulent transactions as compared to the non fraudulent one's is almost null. It makes it so difficult for us to classify the test data.\n",
        "\n",
        "Remember, Rule 1 of the dataset is that the predicted value should be somewhat equally divided between the two classes!\n",
        "\n",
        "Anyway, lets see how well we are able to perform!"
      ],
      "metadata": {
        "id": "oKXKedAeQ56H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "iftf8cT4Q56I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fraud to NonFraud Ratio of {:.3f}%\".format(492/284315*100))"
      ],
      "metadata": {
        "trusted": true,
        "id": "khBghDebQ56I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data.Amount[data.Class == 0], label = 'Fraud', shade=True)\n",
        "sns.kdeplot(data.Amount[data.Class == 1], label = 'NonFraud', shade=True)\n",
        "plt.xlabel('Amount');"
      ],
      "metadata": {
        "trusted": true,
        "id": "MSQkLJh4Q56I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like there a lot more instances of small fraud amounts than really large ones."
      ],
      "metadata": {
        "id": "AAiv0cBAQ56J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data.Time[data.Class == 0], label = 'Fraud', shade=True)\n",
        "sns.kdeplot(data.Time[data.Class == 1], label = 'NonFraud', shade=True)\n",
        "plt.xlabel('Time')"
      ],
      "metadata": {
        "trusted": true,
        "id": "QgPIIy8sQ56J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that the feature time doesn't seem to have an impact in the frequency of frauds.\n",
        "\n"
      ],
      "metadata": {
        "id": "oUI63WA6Q56J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(data.corr()) # Displaying the Heatmap\n",
        "\n",
        "plt.title('Heatmap correlation')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2jb6tTEvQ56J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can notice, most of the features are not correlated with each other.\n",
        "\n",
        "What can generally be done on a massive dataset is a dimension reduction. By picking the most important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.\n"
      ],
      "metadata": {
        "id": "QVIH_Z6oQ56K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the columns from the dataframe\n",
        "columns = data.columns.tolist()\n",
        "\n",
        "# filter the columns to remove the data we do not want\n",
        "columns = [c for c in columns if c not in ['Class']]\n",
        "\n",
        "# store the variable we will be predicting on which is class\n",
        "target = 'Class'\n",
        "\n",
        "# X includes everything except our class column\n",
        "X = data[columns]\n",
        "# Y includes all the class labels for each sample\n",
        "# this is also one-dimensional\n",
        "Y = data[target]\n",
        "\n",
        "# print the shapes of X and Y\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mCf4sIqeQ56K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor"
      ],
      "metadata": {
        "trusted": true,
        "id": "yPNrB6QmQ56K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the number of fraud cases\n",
        "fraud = data[data['Class'] == 1]\n",
        "valid = data[data['Class'] == 0]\n",
        "\n",
        "outlier_fraction = len(fraud) / float(len(valid))\n",
        "print(outlier_fraction)\n",
        "\n",
        "print('Fraud Cases: {}'.format(len(fraud)))\n",
        "print('Valid Cases: {}'.format(len(valid)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "KKschfNQQ56K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = 1\n",
        "\n",
        "# define the outlier detection methods\n",
        "classifiers = {\n",
        "    # contamination is the number of outliers we think there are\n",
        "    'Isolation Forest': IsolationForest(max_samples = len(X),\n",
        "                                       contamination = outlier_fraction,\n",
        "                                       random_state = state),\n",
        "    # number of neighbors to consider, the higher the percentage of outliers the higher you want to make this number\n",
        "    'Local Outlier Factor': LocalOutlierFactor(\n",
        "    n_neighbors = 20,\n",
        "    contamination = outlier_fraction)\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "_le_Vdh-Q56K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_outliers = len(fraud)\n",
        "\n",
        "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
        "\n",
        "    # fit the data and tag outliers\n",
        "    if clf_name == 'Local Outlier Factor':\n",
        "        y_pred = clf.fit_predict(X)\n",
        "        scores_pred = clf.negative_outlier_factor_\n",
        "    else:\n",
        "        clf.fit(X)\n",
        "        scores_pred = clf.decision_function(X)\n",
        "        y_pred = clf.predict(X)\n",
        "\n",
        "\n",
        "\n",
        "# reshape the prediction values to 0 for valid and 1 for fraud\n",
        "    y_pred[y_pred == 1] = 0\n",
        "    y_pred[y_pred == -1] = 1\n",
        "\n",
        "    # calculate the number of errors\n",
        "    n_errors = (y_pred != Y).sum()\n",
        "\n",
        "    # classification matrix\n",
        "    print('{}: {}'.format(clf_name, n_errors))\n",
        "    print(accuracy_score(Y, y_pred))\n",
        "    print(classification_report(Y, y_pred))"
      ],
      "metadata": {
        "trusted": true,
        "id": "8AFSV7asQ56L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at precision for fraudulent cases (1) lets us know the percentage of cases that are getting correctly labeled. 'Precision' accounts for false-positives. 'Recall' accounts for false-negatives. Low numbers could mean that we are constantly calling clients asking them if they actually made the transaction which could be annoying.\n",
        "\n",
        "Goal: To get better percentages.\n",
        "\n",
        "Our Isolation Forest method (which is Random Forest based) was able to produce a better result. Looking at the f1-score 26% (or approx. 30%) of the time we are going to detect the fraudulent transactions."
      ],
      "metadata": {
        "id": "pSD5oWbkQ56L"
      }
    }
  ]
}